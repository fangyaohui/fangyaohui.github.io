<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>L5-大模型预训练-并行训练和框架 | 孤岛</title><meta name="author" content="徐行"><meta name="copyright" content="徐行"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="L5-大模型预训练-并行训练和框架参考资料 从零预训练一个自己的大模型 [大语言模型的预训练[1]:基本概念原理、神经网络的语言模型、Transformer模型原理详解、Bert模型原理介绍(https:&#x2F;&#x2F;cloud.tencent.com&#x2F;developer&#x2F;article&#x2F;2303090)  大纲5.3****大模型预训练l 预训练方法 l 预训练数据准备与清洗 l Scaling Law简">
<meta property="og:type" content="article">
<meta property="og:title" content="L5-大模型预训练-并行训练和框架">
<meta property="og:url" content="http://example.com/2025/02/26/61%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/index.html">
<meta property="og:site_name" content="孤岛">
<meta property="og:description" content="L5-大模型预训练-并行训练和框架参考资料 从零预训练一个自己的大模型 [大语言模型的预训练[1]:基本概念原理、神经网络的语言模型、Transformer模型原理详解、Bert模型原理介绍(https:&#x2F;&#x2F;cloud.tencent.com&#x2F;developer&#x2F;article&#x2F;2303090)  大纲5.3****大模型预训练l 预训练方法 l 预训练数据准备与清洗 l Scaling Law简">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/18.png">
<meta property="article:published_time" content="2025-02-26T10:58:17.000Z">
<meta property="article:modified_time" content="2025-02-26T12:11:49.982Z">
<meta property="article:author" content="徐行">
<meta property="article:tag" content="大模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/18.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "L5-大模型预训练-并行训练和框架",
  "url": "http://example.com/2025/02/26/61%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/",
  "image": "http://example.com/img/18.png",
  "datePublished": "2025-02-26T10:58:17.000Z",
  "dateModified": "2025-02-26T12:11:49.982Z",
  "author": [
    {
      "@type": "Person",
      "name": "徐行",
      "url": "http://example.com/"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/02/26/61%20%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@700&amp;display=swap" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'L5-大模型预训练-并行训练和框架',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/img/18.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">孤岛</span></a><a class="nav-page-title" href="/"><span class="site-name">L5-大模型预训练-并行训练和框架</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">L5-大模型预训练-并行训练和框架</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-26T10:58:17.000Z" title="发表于 2025-02-26 18:58:17">2025-02-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-02-26T12:11:49.982Z" title="更新于 2025-02-26 20:11:49">2025-02-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="L5-大模型预训练-并行训练和框架"><a href="#L5-大模型预训练-并行训练和框架" class="headerlink" title="L5-大模型预训练-并行训练和框架"></a>L5-大模型预训练-并行训练和框架</h1><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.swanlab.cn/zh/examples/pretrain_llm.html">从零预训练一个自己的大模型</a></li>
<li>[大语言模型的预训练[1]:基本概念原理、神经网络的语言模型、Transformer模型原理详解、Bert模型原理介绍(<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2303090">https://cloud.tencent.com/developer/article/2303090</a>)</li>
</ul>
<h2 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h2><h3 id="5-3-大模型预训练"><a href="#5-3-大模型预训练" class="headerlink" title="5.3****大模型预训练"></a><strong>5.3****大模型预训练</strong></h3><p>l 预训练方法</p>
<p>l 预训练数据准备与清洗</p>
<p>l Scaling Law简介</p>
<p>l 二次预训练的灾难性遗忘</p>
<p>l 二次预训练的通用数据混合与数据配比</p>
<h3 id="5-5-并行训练和框架"><a href="#5-5-并行训练和框架" class="headerlink" title="5.5****并行训练和框架"></a><strong>5.5****并行训练和框架</strong></h3><p>l 并行训练背景</p>
<p>l 数据并行</p>
<p>l 模型并行+流水线并行</p>
<p>l 张量并行</p>
<p>l 大模型分布式训练框架简介：DeepSpeed</p>
<p>l DeepSpeed 实践（实际效果）</p>
<h2 id="大语言模型预训练"><a href="#大语言模型预训练" class="headerlink" title="大语言模型预训练"></a>大语言模型预训练</h2><p>预训练（Pre-training）是语言模型学习的初始阶段。在预训练期间，模型会接触到大量未标记的文本数据，例如书籍、文章和网站。在大量未标记文本数据上训练语言模型。比如说在包含数百万本书、文章和网站的数据集上预训练像 GPT-3 这样的语言模型。预训练目标是捕获文本语料库中存在的底层模式、结构和语义知识。</p>
<p>原文链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/ARPOSPF/article/details/131431885">https://blog.csdn.net/ARPOSPF/article/details/131431885</a></p>
<h3 id="预训练方法"><a href="#预训练方法" class="headerlink" title="预训练方法"></a>预训练方法</h3><p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2303090">https://cloud.tencent.com/developer/article/2303090</a></p>
<p>预训练属于迁移学习的范畴。现有的神经网络在进行训练时，一般基于反向传播（Back Propagation，BP）算法，先对网络中的参数进行随机初始化，再利用随机梯度下降（Stochastic Gradient Descent，SGD）等优化算法不断优化模型参数。而预训练的思想是，模型参数不再是随机初始化的，而是通过一些任务进行预先训练，得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练。</p>
<p>预训练将大量低成本收集的训练数据放在一起，经过某种预训练的方法去学习其中的共性，然后将其中的共性 “移植” 到特定任务的模型中，再使用相关特定领域的少量标注数据进行 “微调”。因此，模型只需要从“共性” 出发，去 “学习” 该特定任务的 “特殊” 部分。</p>
<p>例如：让一个完全不懂英文的人去做英文法律文书的关键词提取的工作会完全无法进行，或者说他需要非常多的时间去学习，因为他现在根本看不懂英文。但是，如果让一个英语为母语但没接触过此类工作的人去做这项任务，他可能只需要相对比较短的时间学习如何去提取法律文书的关键词就可以上手这项任务。在这里，英文知识就属于 “共性” 的知识，这类知识不必要只通过英文法律文书的相关语料进行学习，而是可以通过大量英文语料，不管是小说、书籍，还是自媒体，都可以是学习资料的来源。在该例中，让完全不懂英文的人去完成这样的任务，这就对应了传统的直接训练方法，而完全不懂英文的人如果在早期系统学习了英文，再让他去做同样的任务，就对应了 “预训练 + 微调” 的思路，系统的学习英文即为 “预训练” 的过程。</p>
<p>大语言模型的预训练是指搭建一个大的神经网络模型并喂入海量的数据以某种方法去训练语言模型。大语言模型预训练的主要特点是训练语言模型所用的数据量够多、模型够大。</p>
<h4 id="大语言模型的预训练方式有哪些？"><a href="#大语言模型的预训练方式有哪些？" class="headerlink" title="大语言模型的预训练方式有哪些？"></a>大语言模型的预训练方式有哪些？</h4><p>大语言模型的预训练主要分为以下几种方式，各有其特点，适用于不同的任务类型：</p>
<h5 id="（1）掩码语言模型（Masked-Language-Model-MLM）"><a href="#（1）掩码语言模型（Masked-Language-Model-MLM）" class="headerlink" title="（1）掩码语言模型（Masked Language Model, MLM）"></a>（1）掩码语言模型（Masked Language Model, MLM）</h5><ul>
<li><strong>方法</strong>：在预训练时，随机掩盖句子中的一些词，让模型预测这些被掩盖的词。此方法使模型能够同时学习上下文的左、右两侧信息。</li>
<li><strong>代表模型</strong>：BERT 系列（如 BERT、RoBERTa、ALBERT 等）使用掩码语言模型方法。</li>
<li><strong>应用</strong>：MLM 使模型适用于理解和推理类任务，例如情感分析、句子分类、问答系统等。</li>
</ul>
<h5 id="（2）因果语言模型（Causal-Language-Model-CLM）"><a href="#（2）因果语言模型（Causal-Language-Model-CLM）" class="headerlink" title="（2）因果语言模型（Causal Language Model, CLM）"></a>（2）因果语言模型（Causal Language Model, CLM）</h5><ul>
<li><strong>方法</strong>：这种方法基于自回归语言模型原理，通过逐词预测下一个词进行训练。CLM 仅依赖左侧的上下文，适合顺序生成任务。</li>
<li><strong>代表模型</strong>：GPT 系列（如 GPT、GPT-2、GPT-3）使用因果语言模型进行预训练。</li>
<li><strong>应用</strong>：CLM 主要应用于文本生成、对话系统和多轮对话场景，如 ChatGPT 等生成式任务。</li>
</ul>
<h5 id="（3）序列到序列模型（Sequence-to-Sequence-Model-Seq2Seq）"><a href="#（3）序列到序列模型（Sequence-to-Sequence-Model-Seq2Seq）" class="headerlink" title="（3）序列到序列模型（Sequence-to-Sequence Model, Seq2Seq）"></a>（3）序列到序列模型（Sequence-to-Sequence Model, Seq2Seq）</h5><ul>
<li><strong>方法</strong>：使用编码器-解码器架构，输入序列通过编码器转化为上下文表示，再由解码器生成输出序列。Seq2Seq 模型主要适用于输入和输出存在对应关系的任务。</li>
<li><strong>代表模型</strong>：T5、BART 等模型使用 Seq2Seq 架构，将语言理解和生成任务结合。</li>
<li><strong>应用</strong>：该方法广泛用于机器翻译、文本摘要、问答等任务。</li>
</ul>
<h5 id="（4）多任务学习（Multi-task-Learning）"><a href="#（4）多任务学习（Multi-task-Learning）" class="headerlink" title="（4）多任务学习（Multi-task Learning）"></a>（4）多任务学习（Multi-task Learning）</h5><ul>
<li><strong>方法</strong>：模型同时在多个自监督任务上进行训练。例如，T5 模型在多任务框架下使用“填空式”训练任务，将所有任务转化为文本生成问题。</li>
<li><strong>代表模型</strong>：T5、UnifiedQA 等。</li>
<li><strong>应用</strong>：增强模型的多样性，使其能更好地处理跨任务的场景。</li>
</ul>
<hr>
<h3 id="预训练数据准备与清洗"><a href="#预训练数据准备与清洗" class="headerlink" title="预训练数据准备与清洗"></a>预训练数据准备与清洗</h3><p>参考资料：<a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2019-02-21-15">数据清洗&amp;预处理入门完整指南</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/2401_85379281/article/details/143053667">数据清洗与治理：为大模型预训练打造完美数据</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xiaobing259/article/details/138267915#:~:text=%E6%9C%AC%E6%96%87%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D%E4%BA%86%E5%A4%A7,%E5%8F%8A%E6%95%B0%E6%8D%AE%E5%BD%B1%E5%93%8D%E5%88%86%E6%9E%90%E3%80%82">AI大模型探索之路-训练篇6：大语言模型预训练数据准备-预处理</a></p>
<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>数据预处理是建立机器学习模型的第一步（也很可能是最重要的一步），对最终结果有决定性的作用：如果你的数据集没有完成数据清洗和预处理，那么你的模型很可能也不会有效——就是这么简单。</p>
<p>人们通常认为，数据预处理是一个非常枯燥的部分。但它就是「做好准备」和「完全没有准备」之间的差别，也是表现专业和业余之间的差别。就像为度假做好事先准备一样，如果你提前将行程细节确定好，就能够预防旅途变成一场噩梦。</p>
<h4 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h4><p>让我们从导入数据预处理所需要的库开始吧。库是非常棒的使用工具：将输入传递给库，它则完成相应的工作。你可以接触到非常多的库，但在 PYTHON 中，有三个是最基础的库。任何时候，你都很可能最终还是使用到它们。这三个在使用 PYTHON 时最流行的库就是 <strong>Numpy、Matplotlib 和 Pandas</strong>。Numpy 是满足所有数学运算所需要的库，由于代码是基于数学公式运行的，因此就会使用到它。Maplotlib（具体而言，Matplotlib.pyplot）则是满足绘图所需要的库。Pandas 则是最好的导入并处理数据集的一个库。<strong>对于数据预处理而言，Pandas 和 Numpy 基本是必需的</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在导入这些库的时候，赋予其缩写的称呼形式，在之后的使用中，这可以节省一定的时间成本。这一步非常简单，可以用如下方式实现：</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读入数据集</span></span><br><span class="line">dataset = pd.read_csv(<span class="string">&#x27;my_data.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个冒号表示提取数据集的全部行，「:-1」则表示提取除最后一列以外的所有列。最后的「.values」表示希望提取所有的值。</span></span><br><span class="line">X = dataset.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = dataset.iloc[:, <span class="number">3</span>].values</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/fangyaohui/Pictures@main/img/202502262011200.png" alt="image-20241022154619745"></p>
<h5 id="如果有缺失数据会怎么样？"><a href="#如果有缺失数据会怎么样？" class="headerlink" title="如果有缺失数据会怎么样？"></a><strong>如果有缺失数据会怎么样？</strong></h5><p>事实上，我们总会遇到数据缺失。对此，我们可以将存在缺失的行直接删除，但这不是一个好办法，还很容易引发问题。因此需要一个更好的解决方案。最常用的方法是，用其所在列的均值来填充缺失。为此，你可以利用 scikit-learn 预处理模型中的 inputer 类来很轻松地实现。（如果你还不知道，那么我强烈建议你搞明白它：scikit-learn 包含非常棒的机器学习模型）。在机器学习中，你可能并不适应诸如「方法」、「类」和「对象」这些术语。这不是什么大问题！</p>
<ul>
<li>类就是我们希望为某目的所建立的模型。如果我们希望搭建一个棚子，那么搭建规划就是一个类。</li>
<li>对象是类的一个实例。在这个例子中，根据规划所搭建出来的一个棚子就是一个对象。同一个类可以有很多对象，就像可以根据规划搭建出很多个棚子一样。</li>
<li>方法是我们可以在对象上使用的工具，或在对象上实现的函数：传递给它某些输入，它返回一个输出。这就像，当我们的棚子变得有点不通气的时候，可以使用「打开窗户」这个方法。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imputer = Imputer(missing_values = np.nan, strategy = ‘mean’, axis = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">imputer = imputer.fit(X[:, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">X[:, <span class="number">1</span>:<span class="number">3</span>] = imputer.transform(X[:, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> docx2pdf <span class="keyword">import</span> convert</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convert_</span> docx to_pdf(docx file, pdf file):</span><br><span class="line">    convert(docx file, pdf file)</span><br><span class="line"><span class="comment"># 调用示例</span></span><br><span class="line">docx_file =<span class="string">&#x27;example.docx&#x27;</span><span class="comment"># 替换为实际的docx文件路径</span></span><br><span class="line">pdf_file =<span class="string">&#x27;example.pdf&#x27;</span> <span class="comment">#替换为实际的pdf文件路径</span></span><br><span class="line">convert_ docx to_pdf(docx file, pdf file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PlL <span class="keyword">import</span> lmage</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">convertjpg_to_pdf</span>(<span class="params">jpg_file, pdf_file</span>):</span><br><span class="line">    image = lmage.<span class="built_in">open</span>(jpg_file)</span><br><span class="line">    pdffilepath = pdffile</span><br><span class="line">    image.save(pdffile path,<span class="string">&#x27;PDF&#x27;</span>,resolution=<span class="number">100.0</span>, save <span class="built_in">all</span>=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 调用示例</span></span><br><span class="line">jpg_file =<span class="string">&#x27;example.jpg’# 替换为实际的ipg文件路径</span></span><br><span class="line"><span class="string">pdf_file =&#x27;</span>example.pd<span class="string">f&#x27; # 替换为实际的pdf文件路径</span></span><br><span class="line"><span class="string">convertjpg_to_pdf(jpg_file, pdf file)</span></span><br></pre></td></tr></table></figure>

<h5 id="如果包含属性数据，会怎么样呢？"><a href="#如果包含属性数据，会怎么样呢？" class="headerlink" title="如果包含属性数据，会怎么样呢？"></a><strong>如果包含属性数据，会怎么样呢？</strong></h5><p>这是一个好问题。没有办法明确地计算诸如猫、狗、麋鹿的均值。那么可以怎么做呢？可以将属性数据编码为数值！你可能希望使用 sklearn.preprocessing 所提供的 LabelEncoder 类。从你希望进行编码的某列数据入手，调用 label encoder 并拟合在你的数据上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">labelencoder_X = LabelEncoder()</span><br><span class="line">X[:, <span class="number">0</span>] = labelencoder_X.fit_transform(X[:, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p>这就是将第一列中的属性变量替换为数值所需的全部工作了。例如，麋鹿将用 0 表示，狗将用 2 表示，猫将用 3 表示。</p>
<p>标注体系暗含以下信息：所使用的数值层级关系可能会影响模型结果：3 比 0 的数值大，但猫并不一定比麋鹿大。</p>
<p><img src="https://cdn.jsdelivr.net/gh/fangyaohui/Pictures@main/img/202502262011893.png" alt="image-20241022155033442"></p>
<p>我们需要创建哑变量。</p>
<p>我们可以为猫创建一列数据，为麋鹿创建一列数据，……以此类推。然后，将每一列分别以 0&#x2F;1 填充（认为 1&#x3D;Yes，0 &#x3D; No）。这表明，如果原始列的值为猫，那么就会在麋鹿一列得到 0，狗一列得到 0，猫一列得到 1。</p>
<p>看上去非常复杂。输入 OneHotEncoder 吧！</p>
<p>导入编码器，并制定对应列的索引。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">onehotencoder = OneHotEncoder(categorical_features = [<span class="number">0</span>])</span><br><span class="line">X = onehotencoder.fit_transform(X).toarray()</span><br><span class="line">labelencoder_y = LabelEncoder()</span><br><span class="line">y = labelencoder_y.fit_transform(y)</span><br></pre></td></tr></table></figure>

<h4 id="训练集与测试集的划分"><a href="#训练集与测试集的划分" class="headerlink" title="训练集与测试集的划分"></a><strong>训练集与测试集的划分</strong></h4><p>现在，你可以开始将数据集划分为训练集和测试集了。这已经在之前的图像分类教程一文中论述过了。不过记得，一定要将你的数据分为训练集和测试集，永远不要用测试集来训练！需要避免<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/cgi-bin/appmsg?t=media/appmsg_edit&action=edit&type=10&appmsgid=503273714&isMul=1&token=1842803922&lang=zh_CN">过拟合</a>（可以认为，过拟合就像在一次测验前，记忆了许多细节，但没有理解其中的信息。如果只是记忆细节，那么当你自己在家复习知识卡片时，效果会很好，但在所有会考察新信息的真实测验中，都会不及格。）</p>
<p>现在，我们有了需要学习的模型。模型需要在数据上训练，并在另外的数据上完成测试。对训练集的记忆并不等于学习。模型在训练集上学习得越好，就应该在测试集给出更好的预测结果。过拟合永远都不是你想要的结果，学习才是！</p>
<p>首先，导入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure>

<p>现在，可以创建 X_train、X_test、y_train 和 y_test 集合了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span><br></pre></td></tr></table></figure>

<p>一种常见的方法是将数据集按 80&#x2F;20 进行划分，其中 80% 的数据用作训练，20% 的数据用作测试。这也是为何指定 test_size 为 0.2 的原因。你也可以根据自己的需求来任意划分。你并不需要设置 random_state，这里设置的原因是为了可以完全复现结果。</p>
<h4 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a><strong>特征缩放</strong></h4><p>什么是特征缩放？为什么需要特征缩放？</p>
<p>看看我们的数据。我们有一列动物年龄，范围是 4~17，还有一列动物价值，范围是$48,000-$83,000。价值一栏的数值不仅远大于年龄一栏，而且它还包含更加广阔的数据范围。这表明，欧式距离将完全由价值这一特征所主导，而忽视年龄数据的主导效果。如果欧式距离在特定机器学习模型中并没有具体作用会怎么样？缩放特征将仍能够加速模型，因此，你可以在数据预处理中，加入特征缩放这一步。</p>
<p>特征缩放的方法有很多。但它们都意味着我们将所有的特征放在同一量纲上，进而没有一个会被另一个所主导。</p>
<p>导入相关库开始：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import StandardScaler</span><br></pre></td></tr></table></figure>

<p>创建一个需要缩放对象并调用 Standard Scaler</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc_X = StandardScaler()</span><br></pre></td></tr></table></figure>

<p>直接在数据集上进行拟合以及变换。获取对象并应用方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train = sc_X.fit_transform(X_train)</span><br><span class="line">X_test = sc_X.transform(X_test)</span><br></pre></td></tr></table></figure>

<p>不需要在测试集上进行拟合，只进行变换。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sc_y = StandardScaler()</span><br><span class="line">y_train = sc_y.fit_transform(y_train)</span><br></pre></td></tr></table></figure>

<p><strong>对于哑变量而言，是否需要进行缩放？</strong></p>
<p>对于这个问题，有些人认为需要，有些则认为不需要。这取决于你对模型可解释性的看重诚度。将所有数据缩放至同一量纲固然有好处，但缺点是，这丢失了解释每个观测样本归属于哪个变量的便捷性。</p>
<p>对于 Y 呢？如果因变量是 0 和 1，那么并不需要进行特征缩放。这是一个具有明确相关值的分类问题。但如果其取值范围非常大，那么答案是你需要做缩放。</p>
<p>恭喜你，你已经完成了数据预处理的工作！</p>
<p>通过少量的几行代码，你已经领略了数据清洗和预处理的基础。毫无疑问，在数据预处理这一步中，你可以加入很多自己的想法：你可能会想如何填充缺失值。思考是否缩放特征以及如何缩放特征？是否引入哑变量？是否要对数据做编码？是否编码哑变量……有非常多需要考虑的细节。现在，你已经完全了解了这些，可以亲自动手试试了，准备数据吧！</p>
<hr>
<h3 id="Scaling-Law简介"><a href="#Scaling-Law简介" class="headerlink" title="Scaling Law简介"></a>Scaling Law简介</h3><p>参考资料：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27590277/article/details/134544174">解析大模型中的Scaling Law</a></p>
<h4 id="前言-1"><a href="#前言-1" class="headerlink" title="前言"></a>前言</h4><p>在大模型的研发中，通常会有下面一些需求：</p>
<ul>
<li>计划训练一个10B的模型，想知道至少需要多大的数据？</li>
<li>收集到了1T的数据，想知道能训练一个多大的模型？</li>
<li>老板准备1个月后开发布会，能用的资源是100张A100，那应该用多少数据训一个多大模型最终效果最好？</li>
<li>老板对现在10B的模型不满意，想知道扩大到100B模型的效果能提升到多少？</li>
</ul>
<p>以上这些问题都可以基于Scaling Law的理论进行回答。</p>
<h4 id="核心结论"><a href="#核心结论" class="headerlink" title="核心结论"></a>核心结论</h4><p>大模型的Scaling Law是<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=OpenAI&spm=1001.2101.3001.7020">OpenAI</a>在2020年提出的概念[1]，具体如下:</p>
<ol>
<li>对于Decoder-only的模型，计算量(Flops), 模型参数量, 数据大小(token数)，三者满足: 。(推导见本文最后)</li>
<li>模型的最终性能**「主要与」**计算量，模型参数量和数据大小三者相关，而与模型的具体结构(层数&#x2F;深度&#x2F;宽度)基本无关。</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/fangyaohui/Pictures@main/img/202502262011907.png" alt="e005326347d9c36bf43da1175adac2ae.png"></p>
<blockquote>
<blockquote>
<p>固定模型的总参数量，调整层数&#x2F;深度&#x2F;宽度，不同模型的性能差距很小，大部分在2%以内</p>
</blockquote>
</blockquote>
<p>尺度定律（Scaling laws）是一种描述系统随着规模的变化而发生的规律性变化的数学表达。这些规律通常表现为一些可测量的特征随着系统大小的增加而呈现出一种固定的比例关系。尺度定律在不同学科领域中都有广泛的应用，包括物理学、生物学、经济学等。</p>
<p>有趣的是，OpenAI的研究者在2020年发现，大语言模型也遵循着尺度定律[1]。</p>
<p>大语言模型的尺度定律描述的是模型的性能 𝐿 ，模型的参数量大小 𝑁 ，训练模型的数据大小 𝐷 以及训练模型使用的计算量 𝐶 之间的关系。需要注意的是，这里的尺度定律默认要求大语言模型使用的是Transformer的解码器结构。</p>
<p>模型的性能 𝐿 是指模型在测试集上的交叉熵损失：</p>
<p><img src="https://cdn.jsdelivr.net/gh/fangyaohui/Pictures@main/img/202502262011983.png" alt="image-20241022160324756"></p>
<p>𝐷 表示token字典表， 𝑇 表示文本样本被划分为token后的长度。值得注意的是，这里的数学表达进行了一定的简化，仅针对单个文本样本。实际上，测试集由多个文本样本组成。</p>
<p>模型的参数量大小 𝑁 是除了静态编码矩阵和位置编码外的参数。</p>
<p>训练数据大小 𝐷 指的是在训练过程中使用的token数量。通常情况下， 𝐷 等于 𝐵𝑆 ，其中 𝐵 代表使用梯度下降法时的批量大小（Batch Size）， 𝑆 表示参数迭代的次数（Step）。</p>
<p>训练模型使用的计算量 𝐶 是指训练模型时，使用的浮点运算次数。每训练一个token会涉及一次前向传播，一次反向传播，在大语言模型的训练中，反向传播的浮点运算次数约为前向传播的两倍。需要注意的是，和参数量大小 𝑁 类似，这里的浮点运算次数需要排除掉静态编码和位置编码。</p>
<p>在使用Transformer的解码器结构训练模型时，我们可以得到如下的关于 𝐶,𝑁,𝐷 之间的近似计算关系：</p>
<p><img src="https://cdn.jsdelivr.net/gh/fangyaohui/Pictures@main/img/202502262011605.png" alt="image-20241022160443302"></p>
<p>关于上述公式的推导过程，可参考论文“Scaling Laws for Neural Language Models”的2.1小节，在此不再详述。公式(2)表明，当 𝐶 、 𝑁 、 𝐷 三者中已知其中的两个值时，我们可利用上述公式估算出第三个值。</p>
<p>尺度定律的核心结论可以用下面这句话简单总结：</p>
<p>对于计算量 𝐶 ，模型参数量 𝑁 和数据集大小 𝐷 ，当不受其他两个因素制约时，模型性能 𝐿 与每个因素都呈现 幂律关系。</p>
<p><img src="https://cdn.jsdelivr.net/gh/fangyaohui/Pictures@main/img/202502262011965.png" alt="image-20241022160536432"></p>
<hr>
<h3 id="二次预训练的灾难性遗忘"><a href="#二次预训练的灾难性遗忘" class="headerlink" title="二次预训练的灾难性遗忘"></a>二次预训练的灾难性遗忘</h3><h4 id="1-定义"><a href="#1-定义" class="headerlink" title="1. 定义"></a>1. 定义</h4><p>灾难性遗忘（Catastrophic Forgetting）指的是在神经网络模型进行新的学习任务时，已经学到的旧任务知识迅速丢失的现象。这一问题在深度学习和迁移学习中尤其突出，特别是在进行二次预训练（即在原有预训练基础上进行新任务的训练时）时。</p>
<h4 id="2-理论背景"><a href="#2-理论背景" class="headerlink" title="2. 理论背景"></a>2. 理论背景</h4><p>在机器学习中，模型通过不断调整其权重来学习特定任务。如果一个模型在处理新的数据集或任务时进行训练，优化过程可能会导致模型对旧任务的权重调整，从而导致对之前任务的表现显著下降。这种现象通常可以用以下理论框架进行解释：</p>
<ul>
<li><p><strong>权重重分配</strong>：当模型学习新任务时，它的权重更新可能会朝着优化新任务的方向进行调整，导致对旧任务的表示能力下降。</p>
</li>
<li><p><strong>内存限制</strong>：神经网络的容量是有限的，当向其添加新知识时，可能会导致其存储旧知识的能力下降。这就类似于人类在学习新信息时，旧信息的回忆能力减弱。</p>
</li>
<li><p><strong>冲突</strong>：新任务的数据分布可能与旧任务的数据分布存在差异，导致模型在训练过程中对旧知识产生“冲突”。</p>
</li>
</ul>
<h4 id="二次预训练的背景"><a href="#二次预训练的背景" class="headerlink" title="二次预训练的背景"></a>二次预训练的背景</h4><p>在大语言模型的训练中，模型通常经过一次预训练以学习一般性的语言表示，然后再进行针对特定任务的微调或二次预训练。这种方法利用了大量未标注数据的预训练表示，使模型能够更快地适应特定任务。然而，二次预训练可能会导致模型遗忘原有预训练中学到的通用语言表示，从而影响后续任务的表现。</p>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><p>为了解决灾难性遗忘的问题，研究者提出了一些方法，以下是几种常见的解决方案：</p>
<h5 id="1-正则化方法"><a href="#1-正则化方法" class="headerlink" title="1. 正则化方法"></a>1. 正则化方法</h5><ul>
<li><p><strong>弹性权重固定（Elastic Weight Consolidation, EWC）</strong>：</p>
<ul>
<li>通过引入正则化项来约束模型的权重更新，从而保持旧任务的重要特征。具体做法是在训练新任务时，通过计算重要性矩阵，限制对旧任务重要参数的改变。</li>
</ul>
</li>
<li><p><strong>L2正则化</strong>：</p>
<ul>
<li>在新任务的损失函数中添加L2正则化项，以避免模型对权重的剧烈更新，从而保持旧任务的知识。</li>
</ul>
</li>
</ul>
<h5 id="2-增量学习"><a href="#2-增量学习" class="headerlink" title="2. 增量学习"></a>2. 增量学习</h5><ul>
<li><p><strong>多任务学习</strong>：</p>
<ul>
<li>同时训练多个任务，通过共享特征和参数，减少任务之间的干扰。例如，在进行新任务的训练时，可以保留旧任务的训练数据，以便于模型同时学习。</li>
</ul>
</li>
<li><p><strong>记忆增强网络</strong>：</p>
<ul>
<li>使用记忆网络（Memory-Augmented Networks）来存储旧任务的知识。模型在学习新任务时，可以从记忆中提取旧任务的相关信息，从而减少遗忘。</li>
</ul>
</li>
</ul>
<h5 id="3-数据回放"><a href="#3-数据回放" class="headerlink" title="3. 数据回放"></a>3. 数据回放</h5><ul>
<li><p><strong>经验回放（Experience Replay）</strong>：</p>
<ul>
<li>在训练新任务时，定期将旧任务的数据样本（或生成样本）重新引入训练过程中，以帮助模型保持对旧知识的记忆。</li>
</ul>
</li>
<li><p><strong>生成模型</strong>：</p>
<ul>
<li>使用生成对抗网络（GANs）或其他生成模型生成旧任务的样本，在训练新任务时与新数据一起使用，帮助模型保持旧知识。</li>
</ul>
</li>
</ul>
<h5 id="4-元学习"><a href="#4-元学习" class="headerlink" title="4. 元学习"></a>4. 元学习</h5><ul>
<li><strong>元学习（Meta-Learning）</strong>：<ul>
<li>通过训练一个能够快速适应新任务的模型，可以在学习新任务时利用旧任务的信息。该方法通过优化模型在新任务上的学习速度，从而减少灾难性遗忘的风险。</li>
</ul>
</li>
</ul>
<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>灾难性遗忘是深度学习中的一个重要问题，特别是在二次预训练的背景下。通过使用正则化方法、增量学习、数据回放和元学习等策略，可以有效减轻遗忘现象，确保模型在新任务学习的同时保留对旧知识的理解。未来的研究可以进一步探索更加有效的解决方案，以提高模型在不断变化的任务环境中的适应能力。</p>
<hr>
<h3 id="二次预训练的通用数据混合与数据配比"><a href="#二次预训练的通用数据混合与数据配比" class="headerlink" title="二次预训练的通用数据混合与数据配比"></a>二次预训练的通用数据混合与数据配比</h3><p>二次预训练（或称为“二次训练”）是深度学习和自然语言处理领域中的一个重要概念，尤其是在大型语言模型（LLMs）中。其主要目标是通过在已经预训练的基础上，进一步增强模型的性能，特别是在特定任务或领域的应用中。以下是二次预训练过程中通用数据混合与数据配比的详细介绍。</p>
<h4 id="1-通用数据混合"><a href="#1-通用数据混合" class="headerlink" title="1. 通用数据混合"></a>1. 通用数据混合</h4><p>通用数据混合是指在二次预训练阶段，利用多种类型的数据源进行模型训练，以提升模型的泛化能力和适应性。数据源可以包括：</p>
<ul>
<li><p><strong>预训练数据</strong>：使用已经存在的大规模文本数据集，如Wikipedia、Common Crawl等，这些数据通常包含丰富的语言知识和语境信息。</p>
</li>
<li><p><strong>领域特定数据</strong>：针对特定应用场景或行业（如医学、法律等）的数据集。这些数据通常较小但能提供针对性的知识，使模型更好地适应特定任务。</p>
</li>
<li><p><strong>增强数据</strong>：通过数据增强技术生成的新数据，或通过合成方法（如生成对抗网络）产生的样本。这种数据能帮助模型学习到更广泛的特征。</p>
</li>
</ul>
<h5 id="数据混合的好处"><a href="#数据混合的好处" class="headerlink" title="数据混合的好处"></a>数据混合的好处</h5><ul>
<li><p><strong>提高泛化能力</strong>：通过引入多样化的数据源，模型能够学习到更全面的特征，提高在未知数据上的表现。</p>
</li>
<li><p><strong>减少过拟合</strong>：多样化的数据能有效缓解模型在特定数据集上过拟合的风险。</p>
</li>
<li><p><strong>优化迁移学习效果</strong>：模型可以更好地将学到的知识应用到新任务中，提升迁移学习的效果。</p>
</li>
</ul>
<h4 id="2-数据配比"><a href="#2-数据配比" class="headerlink" title="2. 数据配比"></a>2. 数据配比</h4><p>数据配比是指在进行二次预训练时，各类数据的比例分配。合理的数据配比是确保模型性能的重要因素。数据配比通常考虑以下几个方面：</p>
<ul>
<li><p><strong>预训练数据 vs. 领域特定数据</strong>：如果领域特定数据相对较小，可以增加预训练数据的比例，以确保模型仍能获得丰富的语言知识。相反，如果领域特定数据质量高且涵盖目标任务的重要特征，可以适当增加该类数据的比重。</p>
</li>
<li><p><strong>原始数据 vs. 增强数据</strong>：通常情况下，原始数据能提供更真实的样本，而增强数据可以扩展训练集。数据配比的选择依赖于增强数据的质量和多样性，如果增强数据能有效提升模型性能，可以适当增加其比重。</p>
</li>
<li><p><strong>正负样本比例</strong>：在某些特定任务中（如分类），正负样本的比例需要仔细调整，以避免模型偏向于某一类。例如，在不平衡数据集上，可能需要增加负样本的比例，以确保模型能够识别少数类。</p>
</li>
</ul>
<h5 id="配比策略示例"><a href="#配比策略示例" class="headerlink" title="配比策略示例"></a>配比策略示例</h5><ul>
<li><p><strong>1:1 配比</strong>：预训练数据和领域特定数据各占一半，适用于领域特定数据丰富的情况。</p>
</li>
<li><p><strong>7:3 配比</strong>：预训练数据占比高，适用于领域特定数据稀缺但希望保持模型通用性的情况。</p>
</li>
<li><p><strong>2:1:1 配比</strong>：在有丰富原始数据和增强数据的情况下，原始数据占2，增强数据和领域特定数据各占1，以此确保模型在泛化能力与领域知识之间达到平衡。</p>
</li>
</ul>
<h4 id="3-结论"><a href="#3-结论" class="headerlink" title="3. 结论"></a>3. 结论</h4><p>通用数据混合与数据配比在二次预训练过程中起着关键作用。通过合理的数据选择和比例配置，可以有效提升模型的性能，尤其是在处理特定任务或领域的应用时。选择合适的混合策略和配比需要根据具体应用场景进行实验和调整，以达到最佳效果。</p>
<hr>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></div><div class="post-share"><div class="social-share" data-image="/img/18.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/02/26/59%20Spring%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/" title="59 Spring源码"><img class="cover" src="/img/10.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">59 Spring源码</div></div><div class="info-2"><div class="info-item-1">59 Spring源码前言本文主要借鉴网络上对Spring源码解析的文章，并融入自己的理解后得到的记录文章。 希望在记录的过程中可以对Spring的启动以及相关的思想有一定的了解学习。 也希望在这个学习的过程中能够加深对Spring框架的基础结构，对IoC、Aop和MVC等基础概念有了更深的了解。 参考资料来自于：Spring源码分析：全集整理 Spring Boot 启动流程参考资料：https://blog.csdn.net/qq_36882793/article/details/112305866 我们来看一下在Spring boot中的启动类： 1234567@SpringBootApplicationpublic class SpringbootDemoApplication &#123;    public static void main(String[] args) &#123;        ConfigurableApplicationContext run =...</div></div></div></a><a class="pagination-related" href="/2025/02/26/62%20%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95/" title="62 加密算法"><img class="cover" src="/img/4.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">62 加密算法</div></div><div class="info-2"><div class="info-item-1">62...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">徐行</div><div class="author-info-description">道阻且长</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">20</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/fangyaohui"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#L5-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%92%8C%E6%A1%86%E6%9E%B6"><span class="toc-number">1.</span> <span class="toc-text">L5-大模型预训练-并行训练和框架</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">1.1.</span> <span class="toc-text">参考资料</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E7%BA%B2"><span class="toc-number">1.2.</span> <span class="toc-text">大纲</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.2.1.</span> <span class="toc-text">5.3****大模型预训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%92%8C%E6%A1%86%E6%9E%B6"><span class="toc-number">1.2.2.</span> <span class="toc-text">5.5****并行训练和框架</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.3.</span> <span class="toc-text">大语言模型预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.1.</span> <span class="toc-text">预训练方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E5%BC%8F%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">大语言模型的预训练方式有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%8E%A9%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Masked-Language-Model-MLM%EF%BC%89"><span class="toc-number">1.3.1.1.1.</span> <span class="toc-text">（1）掩码语言模型（Masked Language Model, MLM）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%9B%A0%E6%9E%9C%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88Causal-Language-Model-CLM%EF%BC%89"><span class="toc-number">1.3.1.1.2.</span> <span class="toc-text">（2）因果语言模型（Causal Language Model, CLM）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%88Sequence-to-Sequence-Model-Seq2Seq%EF%BC%89"><span class="toc-number">1.3.1.1.3.</span> <span class="toc-text">（3）序列到序列模型（Sequence-to-Sequence Model, Seq2Seq）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0%EF%BC%88Multi-task-Learning%EF%BC%89"><span class="toc-number">1.3.1.1.4.</span> <span class="toc-text">（4）多任务学习（Multi-task Learning）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E4%B8%8E%E6%B8%85%E6%B4%97"><span class="toc-number">1.3.2.</span> <span class="toc-text">预训练数据准备与清洗</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%BC%E5%85%A5"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">导入</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E6%9C%89%E7%BC%BA%E5%A4%B1%E6%95%B0%E6%8D%AE%E4%BC%9A%E6%80%8E%E4%B9%88%E6%A0%B7%EF%BC%9F"><span class="toc-number">1.3.2.2.1.</span> <span class="toc-text">如果有缺失数据会怎么样？</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E6%9E%9C%E5%8C%85%E5%90%AB%E5%B1%9E%E6%80%A7%E6%95%B0%E6%8D%AE%EF%BC%8C%E4%BC%9A%E6%80%8E%E4%B9%88%E6%A0%B7%E5%91%A2%EF%BC%9F"><span class="toc-number">1.3.2.2.2.</span> <span class="toc-text">如果包含属性数据，会怎么样呢？</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%8E%E6%B5%8B%E8%AF%95%E9%9B%86%E7%9A%84%E5%88%92%E5%88%86"><span class="toc-number">1.3.2.3.</span> <span class="toc-text">训练集与测试集的划分</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">1.3.2.4.</span> <span class="toc-text">特征缩放</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scaling-Law%E7%AE%80%E4%BB%8B"><span class="toc-number">1.3.3.</span> <span class="toc-text">Scaling Law简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%89%8D%E8%A8%80-1"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">核心结论</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E6%AC%A1%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E7%81%BE%E9%9A%BE%E6%80%A7%E9%81%97%E5%BF%98"><span class="toc-number">1.3.4.</span> <span class="toc-text">二次预训练的灾难性遗忘</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%AE%9A%E4%B9%89"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">1. 定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%90%86%E8%AE%BA%E8%83%8C%E6%99%AF"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">2. 理论背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E6%AC%A1%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E8%83%8C%E6%99%AF"><span class="toc-number">1.3.4.3.</span> <span class="toc-text">二次预训练的背景</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-number">1.3.4.4.</span> <span class="toc-text">解决方案</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E6%AD%A3%E5%88%99%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">1.3.4.4.1.</span> <span class="toc-text">1. 正则化方法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E5%A2%9E%E9%87%8F%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.4.4.2.</span> <span class="toc-text">2. 增量学习</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E6%95%B0%E6%8D%AE%E5%9B%9E%E6%94%BE"><span class="toc-number">1.3.4.4.3.</span> <span class="toc-text">3. 数据回放</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4-%E5%85%83%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.4.4.4.</span> <span class="toc-text">4. 元学习</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.3.4.5.</span> <span class="toc-text">结论</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E6%AC%A1%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9A%84%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E6%B7%B7%E5%90%88%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94"><span class="toc-number">1.3.5.</span> <span class="toc-text">二次预训练的通用数据混合与数据配比</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E6%B7%B7%E5%90%88"><span class="toc-number">1.3.5.1.</span> <span class="toc-text">1. 通用数据混合</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B7%B7%E5%90%88%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">1.3.5.1.1.</span> <span class="toc-text">数据混合的好处</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%95%B0%E6%8D%AE%E9%85%8D%E6%AF%94"><span class="toc-number">1.3.5.2.</span> <span class="toc-text">2. 数据配比</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%85%8D%E6%AF%94%E7%AD%96%E7%95%A5%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.3.5.2.1.</span> <span class="toc-text">配比策略示例</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%BB%93%E8%AE%BA"><span class="toc-number">1.3.5.3.</span> <span class="toc-text">3. 结论</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/80-02%20%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABasisFormer%20Attention-based%20Time%20Series%20Forecasting%20with%20Learnable%20and%20Interpretable%20Basis/" title="80-02 BasisFormer:Attention-based Time Series Forecasting with Learnable and Interpretable Basis"><img src="/img/14.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="80-02 BasisFormer:Attention-based Time Series Forecasting with Learnable and Interpretable Basis"/></a><div class="content"><a class="title" href="/2025/02/28/80-02%20%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ABasisFormer%20Attention-based%20Time%20Series%20Forecasting%20with%20Learnable%20and%20Interpretable%20Basis/" title="80-02 BasisFormer:Attention-based Time Series Forecasting with Learnable and Interpretable Basis">80-02 BasisFormer:Attention-based Time Series Forecasting with Learnable and Interpretable Basis</a><time datetime="2025-02-28T14:58:17.000Z" title="发表于 2025-02-28 22:58:17">2025-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/80-03%20%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="80-03 时序预测论文阅读笔记"><img src="/img/6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="80-03 时序预测论文阅读笔记"/></a><div class="content"><a class="title" href="/2025/02/28/80-03%20%E6%97%B6%E5%BA%8F%E9%A2%84%E6%B5%8B%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="80-03 时序预测论文阅读笔记">80-03 时序预测论文阅读笔记</a><time datetime="2025-02-28T14:58:17.000Z" title="发表于 2025-02-28 22:58:17">2025-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/80-01%20%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAdaptive%20Normalization%20for%20Non-stationary%20Time%20Series%20Forecasting%20A%20Temporal%20Slice%20Perspective%20/" title="80-01 Adaptive Normalization for Non-stationary Time Series Forecasting:A Temporal Slice Perspective"><img src="/img/7.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="80-01 Adaptive Normalization for Non-stationary Time Series Forecasting:A Temporal Slice Perspective"/></a><div class="content"><a class="title" href="/2025/02/28/80-01%20%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AAdaptive%20Normalization%20for%20Non-stationary%20Time%20Series%20Forecasting%20A%20Temporal%20Slice%20Perspective%20/" title="80-01 Adaptive Normalization for Non-stationary Time Series Forecasting:A Temporal Slice Perspective">80-01 Adaptive Normalization for Non-stationary Time Series Forecasting:A Temporal Slice Perspective</a><time datetime="2025-02-28T13:58:17.000Z" title="发表于 2025-02-28 21:58:17">2025-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/75-19%20Linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/" title="75-19 Linux文件权限"><img src="/img/20.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="75-19 Linux文件权限"/></a><div class="content"><a class="title" href="/2025/02/28/75-19%20Linux%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/" title="75-19 Linux文件权限">75-19 Linux文件权限</a><time datetime="2025-02-28T10:58:18.000Z" title="发表于 2025-02-28 18:58:18">2025-02-28</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/28/79-01%20%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" title="79-01 装饰器模式"><img src="/img/6.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="79-01 装饰器模式"/></a><div class="content"><a class="title" href="/2025/02/28/79-01%20%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F/" title="79-01 装饰器模式">79-01 装饰器模式</a><time datetime="2025-02-28T10:58:17.000Z" title="发表于 2025-02-28 18:58:17">2025-02-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/img/index_img.png);"><div id="footer-wrap"><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.3.3</a></div><div class="footer_custom_text">道阻且长</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>